#!/usr/bin/env python3
"""
chat_index.py - Index and query VS Code Copilot chat sessions

Reverse-engineered VS Code Chat Storage Format:
==============================================

Location: %APPDATA%/Code/User/workspaceStorage/<workspace_hash>/

Files:
  state.vscdb (SQLite)
    - ItemTable: key-value store
    - Key "chat.ChatSessionStore.index" -> JSON with session metadata
      {
        "version": 1,
        "entries": {
          "<session_uuid>": {
            "sessionId": "<uuid>",
            "title": "<auto-generated-title>",
            "lastMessageDate": <epoch_ms>,
            "initialLocation": "panel",
            "isEmpty": false,
            "timing": {"startTime": <ms>, "endTime": <ms>},  # optional
            "stats": {"fileCount": N, "added": N, "removed": N}  # optional
          }
        }
      }

  chatSessions/<session_uuid>.json
    {
      "version": 2,
      "sessionId": "<uuid>",
      "creationDate": <epoch_ms>,
      "lastMessageDate": <epoch_ms>,
      "customTitle": "<title>",
      "requests": [
        {
          "requestId": "request_<uuid>",
          "timestamp": <epoch_ms>,
          "modelId": "copilot/<model_name>",  # e.g., claude-opus-4.5, gemini-3-pro-preview
          "message": {
            "text": "<user_message>",
            "parts": [...]  # rich content with attachments
          },
          "response": [
            {"kind": "progressMessage", "value": "<thinking>"},
            {"kind": "markdownContent", "value": "<response_markdown>"},
            {"kind": "toolCall", "value": {...}},
            ...
          ],
          "result": {"errorDetails": {...}},  # if failed
          "contentReferences": [...],  # files referenced
          "codeCitations": [...]  # code blocks
        }
      ]
    }

Model IDs observed:
  - copilot/claude-opus-4.5
  - copilot/claude-sonnet-4.5
  - copilot/claude-haiku-4.5
  - copilot/gemini-3-pro-preview
  - copilot/grok-code-fast-1
  - copilot/gpt-5.1, gpt-5.2, gpt-5-mini, gpt-4.1, gpt-5.1-codex
  - copilot/auto

Technical Solution for Sovereign-Scale Chat Querying:
====================================================

1. INDEX STRUCTURE (SQLite with FTS5):
   - sessions: session_id, workspace, title, model, created_at, last_msg_at, request_count
   - requests: request_id, session_id, timestamp, model_id, user_text, response_text
   - requests_fts: FTS5 virtual table for full-text search

2. INCREMENTAL INDEXING:
   - Track file mtimes to only re-index changed sessions
   - ~200MB of chat data -> ~5MB index with FTS

3. QUERY PATTERNS:
   - By model: SELECT * FROM sessions WHERE model LIKE '%opus%'
   - By time: ORDER BY last_msg_at DESC
   - By content: SELECT * FROM requests_fts WHERE requests_fts MATCH 'query'
   - Cross-reference: sessions with both user and agent messages containing X

4. EXPORT FORMATS:
   - Markdown per session (for human review)
   - JSONL per model (for DeepSeek fine-tuning/analysis)
   - Conversation pairs (user->assistant) for behavioral analysis
"""

import sqlite3
import json
from pathlib import Path
from datetime import datetime
from collections import defaultdict
import hashlib
import argparse

WORKSPACE_STORAGE = Path.home() / "AppData/Roaming/Code/User/workspaceStorage"
INDEX_DB = Path(__file__).parent / "enclave" / "chat_index.db"


def init_index_db():
    """Create index database schema."""
    INDEX_DB.parent.mkdir(exist_ok=True)
    conn = sqlite3.connect(INDEX_DB)
    conn.executescript("""
        CREATE TABLE IF NOT EXISTS workspaces (
            ws_hash TEXT PRIMARY KEY,
            path TEXT
        );
        
        CREATE TABLE IF NOT EXISTS sessions (
            session_id TEXT PRIMARY KEY,
            ws_hash TEXT,
            title TEXT,
            created_at INTEGER,
            last_msg_at INTEGER,
            request_count INTEGER,
            models TEXT,  -- JSON array
            size_kb REAL,
            file_mtime REAL,
            FOREIGN KEY (ws_hash) REFERENCES workspaces(ws_hash)
        );
        
        CREATE TABLE IF NOT EXISTS requests (
            request_id TEXT PRIMARY KEY,
            session_id TEXT,
            timestamp INTEGER,
            model_id TEXT,
            user_text TEXT,
            response_text TEXT,
            FOREIGN KEY (session_id) REFERENCES sessions(session_id)
        );
        
        CREATE VIRTUAL TABLE IF NOT EXISTS requests_fts USING fts5(
            user_text, response_text, content=requests, content_rowid=rowid
        );
        
        CREATE INDEX IF NOT EXISTS idx_sessions_last_msg ON sessions(last_msg_at DESC);
        CREATE INDEX IF NOT EXISTS idx_requests_model ON requests(model_id);
        CREATE INDEX IF NOT EXISTS idx_requests_session ON requests(session_id);
    """)
    conn.close()


def find_all_workspaces():
    """Find all workspace storage directories with chat sessions."""
    workspaces = []
    for ws in WORKSPACE_STORAGE.iterdir():
        if ws.is_dir():
            chat_dir = ws / "chatSessions"
            if chat_dir.exists() and list(chat_dir.glob("*.json")):
                workspaces.append((ws.name, ws, chat_dir))
    return workspaces


def load_session_file(path: Path) -> dict | None:
    """Load a chat session JSON file."""
    try:
        with open(path, encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        return None


def extract_response_text(response: list) -> str:
    """Extract readable text from response parts.
    
    Response format (as of 2026):
    - thinking: AI reasoning/reflections (most valuable for search)
    - toolInvocationSerialized: tool calls with arguments
    - markdownContent/progressMessage: legacy formats
    """
    parts = []
    for item in response:
        if isinstance(item, dict):
            kind = item.get('kind', '')
            value = item.get('value', '')
            
            # Legacy formats
            if kind in ('markdownContent', 'progressMessage') and isinstance(value, str):
                parts.append(value)
            
            # Thinking/reasoning - the AI's actual thoughts
            elif kind == 'thinking' and isinstance(value, str) and len(value) > 10:
                parts.append(value)
            
            # Tool invocations - extract terminal commands (especially journal entries)
            elif kind == 'toolInvocationSerialized':
                tool_data = item.get('toolSpecificData', {})
                if tool_data.get('kind') == 'terminal':
                    cmd = tool_data.get('commandLine', {})
                    if isinstance(cmd, dict):
                        cmd_text = cmd.get('original', '')
                        # Index journal entries and other substantial commands
                        if cmd_text and ('journal' in cmd_text.lower() or 'remember' in cmd_text.lower() or len(cmd_text) > 200):
                            parts.append(f"[terminal] {cmd_text}")
    return '\n'.join(parts)


def index_session(conn: sqlite3.Connection, ws_hash: str, session_file: Path, force: bool = False) -> bool:
    """Index a single session file. Returns True if indexed."""
    # Check if already indexed with same mtime
    mtime = session_file.stat().st_mtime
    existing = conn.execute(
        "SELECT file_mtime FROM sessions WHERE session_id = ?",
        (session_file.stem,)
    ).fetchone()
    
    if not force and existing and existing[0] == mtime:
        return False  # Already indexed
    
    data = load_session_file(session_file)
    if not data:
        return False
    
    session_id = data.get('sessionId', session_file.stem)
    requests = data.get('requests', [])
    
    models = set()
    for req in requests:
        models.add(req.get('modelId', 'unknown'))
    
    # Upsert session
    conn.execute("""
        INSERT OR REPLACE INTO sessions 
        (session_id, ws_hash, title, created_at, last_msg_at, request_count, models, size_kb, file_mtime)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
    """, (
        session_id,
        ws_hash,
        data.get('customTitle', 'Untitled'),
        data.get('creationDate'),
        data.get('lastMessageDate'),
        len(requests),
        json.dumps(list(models)),
        session_file.stat().st_size / 1024,
        mtime
    ))
    
    # Delete old requests for this session
    conn.execute("DELETE FROM requests WHERE session_id = ?", (session_id,))
    
    # Index requests
    for req in requests:
        user_text = req.get('message', {}).get('text', '') if isinstance(req.get('message'), dict) else ''
        response_text = extract_response_text(req.get('response', []))
        
        conn.execute("""
            INSERT INTO requests (request_id, session_id, timestamp, model_id, user_text, response_text)
            VALUES (?, ?, ?, ?, ?, ?)
        """, (
            req.get('requestId'),
            session_id,
            req.get('timestamp'),
            req.get('modelId', 'unknown'),
            user_text,
            response_text
        ))
    
    return True


def rebuild_fts(conn: sqlite3.Connection):
    """Rebuild FTS index."""
    conn.execute("INSERT INTO requests_fts(requests_fts) VALUES('rebuild')")


def index_all(force: bool = False):
    """Index all chat sessions across all workspaces."""
    init_index_db()
    conn = sqlite3.connect(INDEX_DB)
    
    workspaces = find_all_workspaces()
    total_indexed = 0
    
    for ws_hash, ws_path, chat_dir in workspaces:
        conn.execute(
            "INSERT OR REPLACE INTO workspaces (ws_hash, path) VALUES (?, ?)",
            (ws_hash, str(ws_path))
        )
        
        for session_file in chat_dir.glob("*.json"):
            if index_session(conn, ws_hash, session_file, force=force):
                total_indexed += 1
    
    if total_indexed > 0:
        rebuild_fts(conn)
    
    conn.commit()
    conn.close()
    
    print(f"Indexed {total_indexed} sessions across {len(workspaces)} workspaces")
    print(f"Index: {INDEX_DB}")


def query_sessions(model: str = None, limit: int = 20, search: str = None):
    """Query indexed sessions."""
    conn = sqlite3.connect(INDEX_DB)
    conn.row_factory = sqlite3.Row
    
    if search:
        # Full-text search
        rows = conn.execute("""
            SELECT DISTINCT s.*, r.user_text, r.response_text
            FROM sessions s
            JOIN requests r ON s.session_id = r.session_id
            WHERE r.request_id IN (
                SELECT request_id FROM requests 
                WHERE user_text LIKE ? OR response_text LIKE ?
            )
            ORDER BY s.last_msg_at DESC
            LIMIT ?
        """, (f'%{search}%', f'%{search}%', limit)).fetchall()
    elif model:
        rows = conn.execute("""
            SELECT * FROM sessions 
            WHERE models LIKE ? 
            ORDER BY last_msg_at DESC LIMIT ?
        """, (f'%{model}%', limit)).fetchall()
    else:
        rows = conn.execute("""
            SELECT * FROM sessions ORDER BY last_msg_at DESC LIMIT ?
        """, (limit,)).fetchall()
    
    conn.close()
    return rows


def export_for_model(model_filter: str, output_path: Path):
    """Export conversations for a specific model as JSONL."""
    conn = sqlite3.connect(INDEX_DB)
    conn.row_factory = sqlite3.Row
    
    rows = conn.execute("""
        SELECT r.*, s.title
        FROM requests r
        JOIN sessions s ON r.session_id = s.session_id
        WHERE r.model_id LIKE ?
        ORDER BY r.timestamp
    """, (f'%{model_filter}%',)).fetchall()
    
    with open(output_path, 'w', encoding='utf-8') as f:
        for row in rows:
            entry = {
                'session': row['session_id'],
                'title': row['title'],
                'timestamp': row['timestamp'],
                'model': row['model_id'],
                'user': row['user_text'],
                'assistant': row['response_text'][:10000]  # Truncate for sanity
            }
            f.write(json.dumps(entry, ensure_ascii=False) + '\n')
    
    conn.close()
    print(f"Exported {len(rows)} requests to {output_path}")


def stats():
    """Print statistics about indexed chats."""
    conn = sqlite3.connect(INDEX_DB)
    
    print("=== CHAT INDEX STATISTICS ===")
    
    # Total sessions
    total = conn.execute("SELECT COUNT(*) FROM sessions").fetchone()[0]
    print(f"Total sessions: {total}")
    
    # By model
    print("\nRequests by model:")
    for row in conn.execute("""
        SELECT model_id, COUNT(*) as cnt 
        FROM requests 
        GROUP BY model_id 
        ORDER BY cnt DESC
    """):
        print(f"  {row[0]}: {row[1]}")
    
    # Total size
    size = conn.execute("SELECT SUM(size_kb) FROM sessions").fetchone()[0] or 0
    print(f"\nTotal chat data: {size/1024:.1f} MB")
    
    # Index size
    print(f"Index size: {INDEX_DB.stat().st_size/1024:.1f} KB")
    
    conn.close()


def main():
    parser = argparse.ArgumentParser(description="Index and query VS Code chat sessions")
    sub = parser.add_subparsers(dest='command')
    
    idx = sub.add_parser('index', help='Index all chat sessions')
    idx.add_argument('--force', '-f', action='store_true', help='Force re-index all sessions')
    
    q = sub.add_parser('query', help='Query sessions')
    q.add_argument('--model', '-m', help='Filter by model (e.g., opus, gemini)')
    q.add_argument('--search', '-s', help='Full-text search')
    q.add_argument('--limit', '-n', type=int, default=20)
    
    e = sub.add_parser('export', help='Export for analysis')
    e.add_argument('--model', '-m', required=True, help='Model filter')
    e.add_argument('--output', '-o', required=True, help='Output JSONL path')
    
    sub.add_parser('stats', help='Show statistics')
    
    args = parser.parse_args()
    
    if args.command == 'index':
        index_all(force=args.force)
    elif args.command == 'query':
        sessions = query_sessions(model=args.model, limit=args.limit, search=args.search)
        for s in sessions:
            dt = datetime.fromtimestamp(s['last_msg_at']/1000).strftime("%Y-%m-%d %H:%M")
            models = json.loads(s['models'])
            model_names = ', '.join(m.split('/')[-1] for m in models)
            print(f"[{dt}] {s['request_count']:3d} reqs | {model_names}")
            print(f"  {s['title'][:70]}")
    elif args.command == 'export':
        export_for_model(args.model, Path(args.output))
    elif args.command == 'stats':
        stats()
    else:
        parser.print_help()


if __name__ == "__main__":
    main()
